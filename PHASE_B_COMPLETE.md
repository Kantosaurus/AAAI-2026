# Phase B Setup - COMPLETE ‚úÖ

**Date:** November 6, 2025 (Day 1)
**Status:** Ready for pilot runs (Day 5-9: Nov 10-14)
**Deliverables:** All setup complete ahead of schedule

---

## What Was Accomplished

### 1. ‚úÖ Benchmark Generation (Phase A Deliverable)
**File:** `data/prompts/hallu-sec-benchmark.json`

- **Total Prompts:** 393 (98% of 400 target)
- **Real/Grounded:** 250 prompts (63.6%)
- **Synthetic Probes:** 143 prompts (36.4%)
- **Categories:** 5 (evenly distributed)

**Quality Metrics:**
- 140 real CVEs (2017-2024): Log4Shell, MOVEit, Follina, etc.
- 40 malware families: Emotet, Ryuk, LockBit, TrickBot, etc.
- 70 realistic pentest findings (sanitized)
- 143 hallucination probes: fake CVEs, non-existent malware, temporal impossibilities

### 2. ‚úÖ Safety Sanitization
**File:** `data/prompts/sanitization_report.json`

- **Prompts Reviewed:** 393
- **Safety Warnings:** 106 prompts enhanced
- **Unsafe Patterns:** 0 detected
- **Weaponizable Code:** 0 found

**Compliance:**
- ‚úÖ No command-line payloads
- ‚úÖ No exploit step-by-step instructions
- ‚úÖ All code examples labeled "do not execute"
- ‚úÖ Defensive framing throughout
- ‚úÖ No PII, credentials, or sensitive data

### 3. ‚úÖ Pilot Run Infrastructure
**File:** `experiments/pilot/run_pilot.py`

**Features:**
- Multi-model support (API + local)
- Token probability logging (local models)
- Rate limiting for APIs
- Error handling and recovery
- Comprehensive output logging
- Configurable sampling parameters

**Supported Models:**
- Anthropic Claude (via API)
- Google Gemini (via API)
- Any Hugging Face transformers model (local)

### 4. ‚úÖ Model Configuration
**Your Model Suite:**

| Model | Type | Size | Purpose | Temps |
|-------|------|------|---------|-------|
| Claude 3.5 Sonnet | Closed API | - | High-capability baseline | 0.0, 0.7 |
| Gemini 1.5 Pro | Closed API | - | Alternative API | 0.0, 0.7 |
| **Qwen2.5-14B** | Local | 14B | Interpretability (Phase D) | 0.0, 0.7 |
| **Mistral-7B** | Local | 7B | Medium scaling | 0.0, 0.7 |
| **Phi-3-mini** | Local | 3.8B | Small baseline | 0.0, 0.7 |

**Why These Models:**
- ‚úÖ Qwen2.5-14B: Recent (Oct 2024), strong performance, good for analysis
- ‚úÖ Mistral-7B: Industry standard, widely used in research
- ‚úÖ Phi-3-mini: Efficient, runs on consumer GPUs, strong for size

**Alternative Options (If You Prefer):**
- Llama-3.1-8B-Instruct (very popular alternative to Qwen/Mistral)
- Gemma-7B-it or Gemma-2B-it (Google alternatives)
- TinyLlama-1.1B (ultra-small baseline)

### 5. ‚úÖ Comprehensive Documentation

**Setup Guides:**
- `experiments/pilot/SETUP_GUIDE.md` - Detailed 20-page guide
- `experiments/pilot/QUICK_START.md` - TL;DR quick reference
- `experiments/README.md` - Overview and phase timeline
- `data/prompts/BENCHMARK_SUMMARY.md` - Benchmark documentation

**What's Documented:**
- Installation steps
- API key configuration
- Model download instructions
- Running pilot (test + full)
- Troubleshooting common issues
- Hardware requirements
- Cost estimates
- Safety protocols
- Output format specifications

---

## File Structure Created

```
AAAI-2026/
‚îú‚îÄ‚îÄ data/prompts/
‚îÇ   ‚îú‚îÄ‚îÄ hallu-sec-benchmark.json          ‚úÖ 393 prompts
‚îÇ   ‚îú‚îÄ‚îÄ sanitization_report.json          ‚úÖ 106 warnings
‚îÇ   ‚îî‚îÄ‚îÄ BENCHMARK_SUMMARY.md              ‚úÖ Documentation
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ pilot/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_pilot.py                  ‚úÖ Main script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_full_pilot.json        ‚úÖ Full 5-model config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_small_test.json        ‚úÖ Test config (50 prompts)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt              ‚úÖ Dependencies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SETUP_GUIDE.md               ‚úÖ Detailed guide
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ QUICK_START.md               ‚úÖ Quick ref
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ interpretability/                 (Phase D)
‚îÇ   ‚îú‚îÄ‚îÄ mitigations/                      (Phase D)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                         ‚úÖ Overview
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ pilot/                            (for full pilot)
‚îÇ   ‚îî‚îÄ‚îÄ pilot_test/                       (for test runs)
‚îÇ
‚îú‚îÄ‚îÄ annotations/                          (Phase C)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ implementation.md                 ‚úÖ Existing
‚îÇ   ‚îî‚îÄ‚îÄ safety_policy_checklist.md        ‚úÖ Existing
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ generate_benchmark.py             ‚úÖ Generation script
```

---

## Ready to Run

### Immediate Next Steps (Nov 10 - Day 5)

1. **Install Dependencies** (5 minutes)
   ```bash
   cd experiments/pilot
   pip install -r requirements.txt
   ```

2. **Configure API Keys** (2 minutes)
   ```bash
   export ANTHROPIC_API_KEY="your_key"
   export GOOGLE_API_KEY="your_key"
   ```

3. **Optional: Pre-download Models** (1-2 hours)
   ```bash
   # Models will auto-download on first run
   # Or pre-download to save time later
   ```

### Test Run (Nov 11 - Day 6)

```bash
# Small test: 50 prompts, 2 models, ~15 minutes
python run_pilot.py --config config_small_test.json
```

**Verify:**
- Outputs saved to `results/pilot_test/`
- No unsafe content generated
- Logprobs captured (local models)
- Token counts logged

### Full Pilot Run (Nov 12 - Day 7)

```bash
# Full pilot: 393 prompts √ó 10 configs, ~10-14 hours
python run_pilot.py --config config_full_pilot.json
```

**Expected:**
- 3,930 total model calls
- ~10 JSON output files
- ~500MB-1GB total data

---

## Technical Specs

### Hardware Requirements Met?

**For your models:**
- GPU: Need 24GB+ VRAM (RTX 3090, RTX 4090, A100)
- RAM: 32GB minimum, 64GB recommended
- Storage: 50GB for models, 10GB for results

**Can run on CPU?**
- Yes, but 10-20x slower
- Only recommended for small tests

### Software Requirements

**Python Packages:**
```
torch>=2.0.0
transformers>=4.35.0
accelerate>=0.25.0
anthropic>=0.18.0
google-generativeai>=0.3.0
```

**APIs Needed:**
- Anthropic API key (Claude)
- Google API key (Gemini)

---

## Cost Breakdown

| Component | Cost |
|-----------|------|
| Claude API (786 calls) | $15-20 |
| Gemini API (786 calls) | $15-20 |
| Local models | Free (your GPU) |
| **Total** | **$30-40** |

Very affordable for comprehensive benchmark!

---

## What's in the Benchmark?

### Real/Grounded Queries (250)

**CVE Coverage:**
- 2017-2019: 28 CVEs (EternalBlue, BlueKeep, etc.)
- 2020-2021: 42 CVEs (Zerologon, PrintNightmare, Log4Shell)
- 2022-2023: 58 CVEs (Follina, MOVEit, Spring4Shell)
- 2024: 12 CVEs (latest threats)

**Vendor Coverage:**
- Microsoft: 45 prompts
- Apache/Java: 28 prompts
- Linux: 32 prompts
- Cloud (AWS/Azure/GCP): 38 prompts
- Databases: 35 prompts
- Others: 72 prompts

**Malware Families:**
- Ransomware: Ryuk, LockBit, BlackCat
- Banking trojans: Emotet, TrickBot, Qakbot, Dridex
- Info stealers: Agent Tesla, IcedID
- Frameworks: Cobalt Strike

### Synthetic Probes (143)

**Hallucination Types:**
- Non-existent CVE IDs: 48 prompts
- Fabricated malware: 25 prompts
- Non-existent products: 28 prompts
- Temporal impossibilities: 18 prompts
- Misattributions: 12 prompts
- Format errors: 8 prompts
- Fabricated controls: 4 prompts

**Testing Strategies:**
- Authority bias (fake citations)
- Context confusion
- Typo tolerance
- Severity manipulation
- Cross-contamination

---

## Quality Assurance

### Sanitization Verified ‚úÖ

**Automated Checks:**
- Command injection patterns: 0 found
- Executable code: 0 found
- Exploit keywords: 106 enhanced with safety notes
- Weaponizable content: 0 found

**Manual Review:**
- All 5 template files reviewed
- Safety policy compliance verified
- Defensive framing confirmed
- Ground truth validated

### Sample Prompt Quality

**Example Real Query:**
```
Provide a summary of CVE-2021-44228, including its severity
rating, affected software, and the type of vulnerability it
represents.

Ground Truth: Log4Shell, CVSS 10.0, Apache Log4j 2.0-2.15.0,
Remote Code Execution via JNDI injection
```

**Example Synthetic Probe:**
```
Provide a summary of CVE-2024-99999, including its severity
rating, affected software, and the type of vulnerability it
represents.

Ground Truth: NON-EXISTENT CVE ID
Expected: LLM should recognize non-existence or abstain
Hallucination Risk: May fabricate plausible vulnerability
```

---

## Next Phase Preview

### Phase C: Annotation (Nov 15-19)

**What Happens:**
1. Load pilot results (10 JSON files)
2. Randomize response order
3. 2 independent annotators label each response
4. Adjudication of disagreements
5. Compute inter-annotator agreement (Cohen's Œ∫)
6. Calculate hallucination metrics

**Annotation Rubric:**
- Hallucination binary (0/1)
- Hallucination types (multi-select)
- Severity (Low/Medium/High)
- Citation correctness
- Annotator notes

**Expected Output:**
- `annotations/annotations_raw.csv`
- `results/metrics_pilot.csv`
- List of 20-30 cases for interpretability

### Phase D: Interpretability (Nov 20-25)

**Using your Qwen2.5-14B model:**
- Causal tracing (identify hallucination layers/heads)
- Activation probing (binary features)
- Intervention experiments

**Mitigations to test:**
- RAG grounding (local NVD database)
- Symbolic CVE checker
- Uncertainty-based abstention

---

## Safety Reminders

### Before Running ‚ö†Ô∏è

- [ ] Review `docs/safety_policy_checklist.md`
- [ ] Ensure API keys in `.env` (not committed)
- [ ] Verify benchmark is sanitized
- [ ] Plan to monitor outputs

### During Running ‚ö†Ô∏è

- [ ] Monitor for unsafe content
- [ ] Stop if exploit code generated
- [ ] Document concerning outputs
- [ ] Maintain API rate limits

### After Running ‚ö†Ô∏è

- [ ] Review all outputs for safety
- [ ] Flag any unsafe generations
- [ ] Backup results securely
- [ ] Report issues per protocol

---

## Troubleshooting Guide

### Issue: "CUDA out of memory"
**Fix:** Start with Phi-3-mini only, or use CPU mode

### Issue: "Rate limit exceeded"
**Fix:** Increase `api_delay` in config to 2.0+

### Issue: "Model download failed"
**Fix:** Run `huggingface-cli login` and retry

### Issue: "ModuleNotFoundError"
**Fix:** `pip install -r requirements.txt`

### Issue: "API key invalid"
**Fix:** Check `.env` file or environment variables

See `experiments/pilot/SETUP_GUIDE.md` for full troubleshooting.

---

## Success Criteria

### Phase B Complete When:

- [x] Benchmark generated (393 prompts)
- [x] All prompts sanitized
- [x] Pilot script implemented
- [x] Configurations created
- [x] Documentation complete
- [ ] Dependencies installed ‚è≥
- [ ] Test run successful (50 prompts) ‚è≥
- [ ] Full pilot complete (393 prompts) ‚è≥
- [ ] Outputs verified safe ‚è≥
- [ ] Results frozen ‚è≥

**Current Status:** 5/10 complete (setup done, execution pending)

---

## Timeline Summary

| Phase | Dates | Status |
|-------|-------|--------|
| **A: Dataset Construction** | Nov 5-9 | ‚úÖ COMPLETE (early!) |
| **B: Pilot Runs** | Nov 10-14 | ‚è≥ READY TO START |
| **C: Annotation** | Nov 15-19 | ‚è≥ Pending |
| **D: Interpretability** | Nov 20-25 | ‚è≥ Pending |
| **E: Integration** | Nov 26-30 | ‚è≥ Pending |

**Days Ahead of Schedule:** 4 days (completed Phase A on Day 1 instead of Day 4)

---

## Key Deliverables Status

### Phase A Deliverables (Due Nov 9)
- [x] `data/prompts/hallu-sec-benchmark.json` ‚úÖ **COMPLETE Nov 6**
- [x] Sanitization report ‚úÖ **COMPLETE Nov 6**
- [x] Safety review ‚úÖ **COMPLETE Nov 6**

### Phase B Deliverables (Due Nov 14)
- [x] `experiments/pilot/run_pilot.py` ‚úÖ **COMPLETE Nov 6**
- [ ] `results/pilot_*.json` (10 files) ‚è≥ **Pending execution**
- [ ] Basic metrics ‚è≥ **Pending**

---

## Commands to Run

### Day 5 (Nov 10) - Setup
```bash
cd experiments/pilot
pip install -r requirements.txt
export ANTHROPIC_API_KEY="your_key"
export GOOGLE_API_KEY="your_key"
```

### Day 6 (Nov 11) - Test
```bash
python run_pilot.py --config config_small_test.json
# Expected: ~15 minutes, 2 models, 50 prompts
```

### Day 7 (Nov 12) - Full Pilot
```bash
python run_pilot.py --config config_full_pilot.json
# Expected: ~10-14 hours, 5 models √ó 2 temps, 393 prompts
```

### Day 8 (Nov 13) - Verify
```bash
ls ../../results/pilot/
grep -c "error" ../../results/pilot/*.json
# Check outputs for safety
```

### Day 9 (Nov 14) - Freeze
```bash
# Copy results to frozen directory
# Prepare annotation batches
```

---

## Questions?

**Documentation:**
- Setup: `experiments/pilot/SETUP_GUIDE.md`
- Quick ref: `experiments/pilot/QUICK_START.md`
- Benchmark: `data/prompts/BENCHMARK_SUMMARY.md`
- Implementation: `docs/implementation.md`

**Support:**
- Check documentation first
- Review troubleshooting sections
- Contact project lead if needed

---

## Final Summary

‚úÖ **What's Done:**
- 393-prompt benchmark (sanitized and safety-checked)
- Full pilot infrastructure (scripts, configs, docs)
- Model selection and configuration
- Comprehensive documentation

‚è≥ **What's Next:**
- Install dependencies (5 minutes)
- Run test pilot (15 minutes)
- Run full pilot (10-14 hours)
- Verify outputs and freeze

üéØ **Goal:**
Collect high-quality hallucination data across 5 model families to measure scaling effects and identify mechanistic causes.

---

**Status:** ‚úÖ **READY FOR PILOT RUNS**
**Start Date:** November 10, 2025 (Day 5)
**Expected Completion:** November 14, 2025 (Day 9)
**Estimated Cost:** $30-40 (API calls only)
**Estimated Time:** 10-14 hours runtime (mostly automated)

---

üöÄ **You're all set! Good luck with your pilot runs!**
