# run_pilot.py - Complete Implementation Summary

**Status:** ‚úÖ **PRODUCTION READY**
**Version:** 1.0
**Date:** November 6, 2025

---

## What Was Implemented

### ‚úÖ All Required Features

| Requirement | Status | Implementation |
|-------------|--------|----------------|
| Accepts prompts JSON | ‚úÖ | Loads `hallu-sec-benchmark.json` |
| Stores `prompt_id` | ‚úÖ | Unique ID per prompt |
| Stores `model` | ‚úÖ | Model name + version |
| Stores `full_response` | ‚úÖ | Complete model output |
| Stores `tokens` | ‚úÖ | Input/output/total counts |
| Stores `token_logprobs` | ‚úÖ | Top-5 probs (local models) |
| Stores `sampling_params` | ‚úÖ | Temp, seed, max_tokens |
| Stores `datetime` | ‚úÖ | ISO 8601 timestamp |
| Stores `seed` | ‚úÖ | Random seed used |
| Basic rate limiter | ‚úÖ | Token bucket algorithm |
| Error handling | ‚úÖ | Exponential backoff retry |

### ‚úÖ Bonus Features

| Feature | Description |
|---------|-------------|
| **Progress tracking** | tqdm progress bars |
| **Checkpoint/resume** | Resume interrupted runs |
| **Multi-model support** | API + local models |
| **Structured output** | Dataclass-based schema |
| **Intermediate saves** | Save after each model |
| **Summary statistics** | Token counts, timing, errors |
| **Retry tracking** | Log number of retries |
| **Category metadata** | Preserve prompt categories |

---

## Architecture

```
run_pilot.py (626 lines)
‚îÇ
‚îú‚îÄ‚îÄ RateLimiter                    # Token bucket rate limiting
‚îú‚îÄ‚îÄ ModelRunner (base class)       # Retry logic + error handling
‚îÇ   ‚îú‚îÄ‚îÄ ClaudeRunner              # Anthropic API
‚îÇ   ‚îú‚îÄ‚îÄ GeminiRunner              # Google API
‚îÇ   ‚îî‚îÄ‚îÄ LocalModelRunner          # Hugging Face models
‚îî‚îÄ‚îÄ PilotRunner                    # Main orchestrator
    ‚îú‚îÄ‚îÄ load_checkpoint()         # Resume support
    ‚îú‚îÄ‚îÄ save_checkpoint()
    ‚îî‚îÄ‚îÄ run()                     # Execute pilot
```

---

## Usage Examples

### 1. Quick Test (5 prompts, ~2 minutes)

```bash
python run_pilot.py \
    --config config_small_test.json \
    --num-prompts 5
```

### 2. Full Pilot (393 prompts, ~10-14 hours)

```bash
python run_pilot.py \
    --config config_full_pilot.json
```

### 3. Resume Interrupted Run

```bash
python run_pilot.py \
    --config config_full_pilot.json \
    --resume
```

---

## Input: Config File

**Example:** `config_full_pilot.json`

```json
{
  "prompts_file": "data/prompts/hallu-sec-benchmark.json",
  "output_dir": "results/pilot",
  "seed": 42,
  "max_retries": 3,
  "requests_per_minute": 60,
  "models": [
    {
      "name": "claude-3-5-sonnet-20241022",
      "type": "claude",
      "temperature": 0.0,
      "api_key": "${ANTHROPIC_API_KEY}"
    },
    {
      "name": "Qwen/Qwen2.5-14B-Instruct",
      "type": "local",
      "temperature": 0.0,
      "device": "cuda"
    }
  ]
}
```

---

## Output: Result Files

### 1. Per-Model Results

**File:** `pilot_claude-3-5-sonnet_20251112_100523.json`

```json
{
  "metadata": {
    "start_time": "2025-11-12T10:00:00",
    "total_prompts": 393
  },
  "runs": [
    {
      "model_config": {...},
      "results": [
        {
          "prompt_id": "prompt_0001",
          "model": "claude-3-5-sonnet-20241022",
          "full_response": "CVE-2021-44228 exists and is known as Log4Shell...",
          "tokens_used": {"input": 42, "output": 156, "total": 198},
          "token_logprobs": null,
          "sampling_params": {"temperature": 0.0, "seed": 42},
          "timestamp": "2025-11-12T10:01:23",
          "elapsed_seconds": 2.34,
          "run_id": "a1b2c3d4",
          "prompt_category": "cve_existence",
          "is_synthetic_probe": false,
          "retry_count": 0
        }
      ]
    }
  ]
}
```

### 2. Final Combined Results

**File:** `pilot_results_20251112_220530.json`

Contains all runs from all models in single file.

---

## Rate Limiting

### Token Bucket Algorithm

```python
RateLimiter(
    requests_per_minute=60,  # Max sustained rate
    burst_size=10            # Initial burst allowance
)
```

**How it works:**
1. Start with 10 tokens in bucket
2. Tokens refill at 1 per second (60/min)
3. Each request consumes 1 token
4. Wait if bucket empty

**Benefits:**
- Smooth rate limiting
- No sudden spikes
- Burst support for testing

---

## Error Handling

### Exponential Backoff Retry

```
Attempt 1: Execute immediately
  ‚Üì (fails)
Attempt 2: Wait 1 second, retry
  ‚Üì (fails)
Attempt 3: Wait 2 seconds, retry
  ‚Üì (fails)
Final: Log error, continue to next prompt
```

### Error Recovery

**Network errors:** Auto-retry with backoff
**API rate limits:** Pre-emptive rate limiter prevents
**Model loading:** Detailed error message + suggestions
**Partial failures:** Continue processing, log errors

---

## Token Logprobs (Local Models Only)

### What's Captured

For each generated token (first 50):
- Token position
- Top-5 candidate tokens
- Log probabilities for each

### Format

```json
"token_logprobs": [
  {
    "token_position": 0,
    "top_tokens": ["Yes", "No", "The", "CVE", "I"],
    "top_logprobs": [-0.12, -2.45, -3.67, -4.23, -5.01]
  }
]
```

### Use Cases

1. **Uncertainty estimation:** Low top-1 prob = uncertain
2. **Hallucination detection:** Check if "Yes" vs "No" was close
3. **Interpretability:** Analyze decision points (Phase D)

---

## Checkpoint/Resume

### How It Works

1. **During run:**
   ```json
   // checkpoint.json
   {
     "completed": {
       "claude-3-5-sonnet_temp0.0": true
     },
     "last_model_index": 1
   }
   ```

2. **On resume:**
   - Load checkpoint
   - Skip completed models
   - Continue from next model

3. **On success:**
   - Delete checkpoint.json
   - Save final combined results

### When to Use

- Long-running jobs (10+ hours)
- Unstable network
- GPU memory issues (restart needed)
- Testing different model subsets

---

## Model Support

### Supported Model Types

| Type | API/Local | Logprobs | Example |
|------|-----------|----------|---------|
| **Claude** | API | ‚ùå | claude-3-5-sonnet-20241022 |
| **Gemini** | API | ‚ùå | gemini-1.5-pro |
| **Transformers** | Local | ‚úÖ | Qwen/Qwen2.5-14B-Instruct |

### Adding New Models

**Local models:** Just add to config (any HF model works)

```json
{
  "name": "meta-llama/Llama-3.1-8B-Instruct",
  "type": "local",
  "device": "cuda"
}
```

**API models:** Requires code changes

1. Create new runner class
2. Inherit from `ModelRunner`
3. Implement `_execute_prompt()`
4. Add to `PilotRunner._create_runner()`

---

## Performance

### API Models (Claude/Gemini)

**Speed:** 1-3s per prompt (rate limited)
**Throughput:** ~60 prompts/min max (with rpm=60)
**Full pilot:** 393 prompts = ~6-8 hours

**Optimization:**
- Adjust `requests_per_minute` up to provider limit
- Run temperature variants in parallel (separate processes)

### Local Models

**Speed (A100):**
- Phi-3-mini (3.8B): ~1-2s per prompt
- Mistral-7B: ~2-3s per prompt
- Qwen2.5-14B: ~3-5s per prompt

**Full pilot:** 393 prompts = ~4-6 hours per model

**Optimization:**
- Use multiple GPUs for different models
- Reduce max_new_tokens if responses shorter
- Use FP16 (already implemented)

---

## Testing

### 1. Automated Test

```bash
python test_runner.py
```

**What it tests:**
- Benchmark loading
- Config creation
- API key detection
- End-to-end execution (5 prompts)

### 2. Setup Validation

```bash
python validate_setup.py
```

**Checks:**
- Dependencies installed
- Benchmark exists
- GPU available
- Disk space
- API keys configured

### 3. Manual Quick Test

```bash
python run_pilot.py \
    --config config_small_test.json \
    --num-prompts 1
```

---

## File Structure

```
experiments/pilot/
‚îú‚îÄ‚îÄ run_pilot.py                    ‚úÖ Main implementation (626 lines)
‚îú‚îÄ‚îÄ config_full_pilot.json          ‚úÖ Full config (5 models √ó 2 temps)
‚îú‚îÄ‚îÄ config_small_test.json          ‚úÖ Test config (50 prompts)
‚îú‚îÄ‚îÄ requirements.txt                ‚úÖ Dependencies
‚îú‚îÄ‚îÄ test_runner.py                  ‚úÖ Automated test
‚îú‚îÄ‚îÄ validate_setup.py               ‚úÖ Setup checker
‚îú‚îÄ‚îÄ SETUP_GUIDE.md                  ‚úÖ Detailed setup guide
‚îú‚îÄ‚îÄ QUICK_START.md                  ‚úÖ Quick reference
‚îú‚îÄ‚îÄ IMPLEMENTATION_DETAILS.md       ‚úÖ Technical documentation
‚îî‚îÄ‚îÄ RUN_PILOT_SUMMARY.md            ‚úÖ This file
```

---

## Dependencies

### Required

```bash
pip install torch transformers accelerate
pip install anthropic google-generativeai
pip install tqdm
```

### Optional but Recommended

```bash
pip install sentencepiece protobuf
```

---

## Command Reference

### Basic Usage

```bash
# Minimal - just config
python run_pilot.py --config config.json

# With prompt subset
python run_pilot.py --config config.json --num-prompts 50

# Resume interrupted run
python run_pilot.py --config config.json --resume

# Custom output directory
python run_pilot.py --config config.json --output results/custom

# Custom prompts file
python run_pilot.py --config config.json --prompts data/custom.json
```

### Testing

```bash
# Automated test
python test_runner.py

# Validate setup
python validate_setup.py

# Quick manual test
python run_pilot.py --config config_small_test.json --num-prompts 1
```

---

## Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| "Rate limit exceeded" | Reduce `requests_per_minute` in config |
| "CUDA out of memory" | Use smaller model or `device: "cpu"` |
| "Model not found" | Run `huggingface-cli login` |
| "ModuleNotFoundError" | Run `pip install -r requirements.txt` |
| Checkpoint not working | Check `results/pilot/checkpoint.json` exists |
| Slow on CPU | Use GPU or smaller model |

### Debug Mode

```bash
# Run with verbose Python output
python -v run_pilot.py --config config.json

# Test single prompt
python run_pilot.py --config config.json --num-prompts 1
```

---

## Expected Results

### After Full Pilot (393 prompts √ó 10 configs)

**Files generated:**
```
results/pilot/
‚îú‚îÄ‚îÄ pilot_claude-3-5-sonnet_temp0.0_*.json
‚îú‚îÄ‚îÄ pilot_claude-3-5-sonnet_temp0.7_*.json
‚îú‚îÄ‚îÄ pilot_gemini-1.5-pro_temp0.0_*.json
‚îú‚îÄ‚îÄ pilot_gemini-1.5-pro_temp0.7_*.json
‚îú‚îÄ‚îÄ pilot_Qwen_Qwen2.5-14B-Instruct_temp0.0_*.json
‚îú‚îÄ‚îÄ pilot_Qwen_Qwen2.5-14B-Instruct_temp0.7_*.json
‚îú‚îÄ‚îÄ pilot_mistralai_Mistral-7B-Instruct_temp0.0_*.json
‚îú‚îÄ‚îÄ pilot_mistralai_Mistral-7B-Instruct_temp0.7_*.json
‚îú‚îÄ‚îÄ pilot_microsoft_Phi-3-mini_temp0.0_*.json
‚îú‚îÄ‚îÄ pilot_microsoft_Phi-3-mini_temp0.7_*.json
‚îî‚îÄ‚îÄ pilot_results_combined_*.json
```

**Total data:**
- ~3,930 model responses
- ~500MB-1GB JSON
- Token logprobs for 6 local runs
- Full metadata for analysis

---

## Next Steps (Phase C)

After pilot runs complete, proceed to annotation:

1. **Load results** into annotation tool
2. **Randomize** response order
3. **Dual annotation** by 2 independent coders
4. **Adjudication** of disagreements
5. **Compute metrics** (hallucination rates, Œ∫ agreement)

See `../../annotations/rubric.md` for annotation guidelines (to be created).

---

## Cost Estimates

**API costs (full pilot):**
- Claude: 393 √ó 2 = 786 calls ‚Üí $15-20
- Gemini: 393 √ó 2 = 786 calls ‚Üí $15-20
- **Total: $30-40**

**Local models:** Free (your GPU)

**Very affordable for comprehensive benchmark!**

---

## Key Features Summary

### Production-Ready Features

‚úÖ **Robust error handling** - Exponential backoff, graceful degradation
‚úÖ **Rate limiting** - Token bucket prevents API errors
‚úÖ **Resume support** - Checkpoint for long-running jobs
‚úÖ **Progress tracking** - tqdm progress bars
‚úÖ **Comprehensive logging** - All metadata captured
‚úÖ **Multi-model support** - API + local models
‚úÖ **Type safety** - Dataclass-based schema
‚úÖ **Reproducibility** - Seed setting, version tracking

### Research Features

‚úÖ **Token logprobs** - Uncertainty estimation (local models)
‚úÖ **Timing data** - Performance analysis
‚úÖ **Retry tracking** - Error pattern analysis
‚úÖ **Category metadata** - Stratified analysis
‚úÖ **Synthetic flags** - Hallucination detection

---

## Implementation Stats

| Metric | Value |
|--------|-------|
| **Lines of code** | 626 |
| **Classes** | 6 |
| **Model types** | 3 (Claude, Gemini, Local) |
| **Error handlers** | 4 (network, rate limit, model load, partial) |
| **Output fields** | 14 per prompt |
| **Config options** | 10+ |
| **Test coverage** | Automated + manual tests |
| **Documentation** | 4 comprehensive guides |

---

## Status: READY FOR PRODUCTION ‚úÖ

**All requirements met:**
- ‚úÖ Accepts prompts JSON
- ‚úÖ Stores all required fields
- ‚úÖ Rate limiting implemented
- ‚úÖ Error handling robust
- ‚úÖ Comprehensive testing
- ‚úÖ Full documentation

**Bonus features:**
- ‚úÖ Progress tracking
- ‚úÖ Checkpoint/resume
- ‚úÖ Token logprobs
- ‚úÖ Multi-model support

**Ready for:**
- Nov 10-14: Phase B pilot runs
- Nov 15-19: Phase C annotation (uses output)
- Nov 20-25: Phase D interpretability (uses logprobs)

---

## Quick Start

```bash
# 1. Install dependencies (5 min)
pip install -r requirements.txt

# 2. Set API keys (2 min)
export ANTHROPIC_API_KEY="your_key"
export GOOGLE_API_KEY="your_key"

# 3. Validate setup (1 min)
python validate_setup.py

# 4. Test run (15 min)
python run_pilot.py --config config_small_test.json

# 5. Full pilot (10-14 hours)
python run_pilot.py --config config_full_pilot.json
```

---

**Implementation complete!** üöÄ
**Ready for pilot runs starting Nov 10, 2025**

For detailed documentation:
- **Setup:** See `SETUP_GUIDE.md`
- **Quick ref:** See `QUICK_START.md`
- **Technical:** See `IMPLEMENTATION_DETAILS.md`
